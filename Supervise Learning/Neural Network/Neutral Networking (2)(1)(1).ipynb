{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac5cc7b",
   "metadata": {},
   "source": [
    "# Mulilayer Neural Network\n",
    "\n",
    "---\n",
    "## Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Types of Neural Network](#type)\n",
    "* [Multi-Layer Neural Network](#multi)\n",
    "* [Formula for Multi-Layered Neural Network](#formula)\n",
    "* [Application](#app)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d73cf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction <a class=\"anchor\" id=\"Introduction\"></a>\n",
    "\n",
    "A series or set of algorithms that try to recognize the underlying relationship in a data set through a definite process that mimics the operation of the human brain is known as Neural Network. Hence, the neural networks could refer to the neurons of the human, either artificial or organic in nature. A neural network can easily adapt to the changing input to achieve or generate the best possible result by the network and does not need to redesign the output criteria.\n",
    "\n",
    "---\n",
    "## Types of Neural Network<a class=\"anchor\" id=\"type\"></a >\n",
    "\n",
    "Neural Networks can be classified into multiple types based on their Layers and depth activation filters, Structure, Neurons used, Neuron density, data flow, and so on. The types of Neural Networks are as follows: \n",
    "\n",
    "* Perceptron\n",
    "* Multi-Layer Perceptron or Multi-Layer Neural Network\n",
    "* Feed Forward Neural Networks\n",
    "* Convolutional Neural Networks\n",
    "* Radial Basis Function Neural Networks\n",
    "* Recurrent Neural Networks\n",
    "* Sequence to Sequence Model\n",
    "* Modular Neural Network\n",
    "\n",
    "---\n",
    "## Multi-Layer Neural Network<a class=\"anchor\" id=\"multi\"></a >\n",
    "\n",
    "To be accurate a fully connected Multi-Layered Neural Network is known as Multi-Layer Perceptron. A Multi-Layered Neural Network consists of multiple layers of artificial neurons or nodes. Unlike Single-Layer Neural Network, in recent times most of the networks have Multi-Layered Neural Network. The following diagram is a visualization of a multi-layer neural network. \n",
    "![image](https://media.geeksforgeeks.org/wp-content/uploads/20200702205951/nn.PNG)\n",
    "\n",
    "### Explanation: \n",
    "Here the nodes marked as “1” are known as bias units. The leftmost layer or Layer 1 is the input layer, the middle layer or Layer 2 is the hidden layer and the rightmost layer or Layer 3 is the output layer. It can say that the above diagram has 3 input units (leaving the bias unit), 1 output unit, and 3 hidden units.\n",
    "A Multi-layered Neural Network is the typical example of the Feed Forward Neural Network. The number of neurons and the number of layers consists of the hyperparameters of Neural Networks which need tuning. In order to find ideal values for the hyperparameters, one must use some cross-validation techniques. Using the Back-Propagation technique, weight adjustment training is carried out.\n",
    "\n",
    "## Formula for Multi-Layered Neural Network<a class=\"anchor\" id=\"formula\"></a >\n",
    "\n",
    "Suppose we have xn inputs(x1, x2….xn) and a bias unit. Let the weight applied be w1, w2…..wn. Then find the summation and bias unit on performing dot product among inputs and weights as:\n",
    "$$r =\\sum_{i=1}^m(x_{i})+bias$$\n",
    "On feeding the r into activation function F(r) we find the output for the hidden layers. For the first hidden layer h1, the neuron can be calculated as: \n",
    "$$h_1^1 = F(r)$$\n",
    "For all the other hidden layers repeat the same procedure. Keep repeating the process until reach the last weight set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460f838",
   "metadata": {},
   "source": [
    "## Application<a class=\"anchor\" id=\"app\"></a >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd8f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\Code\\2022\\5_4_2\\virt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\Code\\2022\\5_4_2\\virt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\Code\\2022\\5_4_2\\virt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\Code\\2022\\5_4_2\\virt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\Code\\2022\\5_4_2\\virt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\Code\\2022\\5_4_2\\virt\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "class MultilayerPerceptron():\n",
    "  \n",
    "    def __init__(self, layers = [784, 60, 60, 10], actFun_type='relu'):\n",
    "        self.actFun_type = actFun_type\n",
    "        self.layers = layers\n",
    "        self.L = len(self.layers)\n",
    "        self.W =[[0.0]]\n",
    "        self.B = [[0.0]]\n",
    "        for i in range(1, self.L):\n",
    "            w_temp = np.random.randn(self.layers[i], self.layers[i-1]) * np.sqrt(2/self.layers[i-1])\n",
    "            b_temp = np.random.randn(self.layers[i], 1) * np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "            self.W.append(w_temp)\n",
    "            self.B.append(b_temp)\n",
    "\n",
    "    def reset_weights(self, layers = [784, 60, 60, 10]):\n",
    "        self.layers = layers\n",
    "        self.L = len(self.layers)\n",
    "        self.W = [[0.0]]\n",
    "        self.B = [[0.0]]\n",
    "        for i in range(1, self.L):\n",
    "            w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "            b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "            self.W.append(w_temp)\n",
    "            self.B.append(b_temp)\n",
    "\n",
    "    def forward_pass(self, p, predict_vector = False):\n",
    "        Z =[[0.0]]\n",
    "        A = [p[0]]\n",
    "        for i in range(1, self.L):\n",
    "            z = (self.W[i] @ A[i-1]) + self.B[i]\n",
    "            a = self.actFun(z, self.actFun_type)\n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "\n",
    "        if predict_vector == True:\n",
    "            return A[-1]\n",
    "        else:\n",
    "            return Z, A\n",
    "\n",
    "    def mse(self, a, y):\n",
    "        return .5*sum((a[i]-y[i])**2 for i in range(10))[0]\n",
    "\n",
    "    def MSE(self, data):\n",
    "        c = 0.0\n",
    "        for p in data:\n",
    "            a = self.forward_pass(p, predict_vector=True)\n",
    "            c += self.mse(a, p[1])\n",
    "        return c/len(data)\n",
    "\n",
    "    def actFun(self, z, type):\n",
    "        if type == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        elif type == 'sigmoid':\n",
    "            return 1.0 / (1.0 + np.exp(-z))\n",
    "        elif type == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def diff_actFun(self, z, type):\n",
    "        if type == 'tanh':\n",
    "            return 1.0 - (np.tanh(z))**2\n",
    "        elif type == 'sigmoid':\n",
    "            return self.actFun(z, type) * (1-self.actFun(z, type))\n",
    "        elif type == 'relu':\n",
    "            return np.where(z > 0, 1.0, 0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def deltas_dict(self, p):\n",
    "        Z, A = self.forward_pass(p)\n",
    "        deltas = dict()\n",
    "        deltas[self.L-1] = (A[-1] - p[1])*self.diff_actFun(Z[-1], self.actFun_type)\n",
    "        for l in range(self.L-2, 0, -1):\n",
    "            deltas[l] = (self.W[l+1].T @ deltas[l+1]) *self.diff_actFun(Z[l], self.actFun_type)\n",
    "\n",
    "        return A, deltas\n",
    "\n",
    "    def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
    "        print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "        for k in range(epochs):\n",
    "            for p in data:\n",
    "                A, deltas = self.deltas_dict(p)\n",
    "                for i in range(1, self.L):\n",
    "                    self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
    "                    self.B[i] = self.B[i] - alpha*deltas[i]\n",
    "        print(f\"{k} Cost = {self.MSE(data)}\")\n",
    "\n",
    "\n",
    "    def mini_batch_gradient_descent(self, data, batch_size = 15, alpha = 0.04, epochs = 3):\n",
    "        print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "        data_length = len(data)\n",
    "        for k in range(epochs):\n",
    "            for j in range(0, data_length-batch_size, batch_size):\n",
    "                delta_list = []\n",
    "                A_list = []\n",
    "                for p in data[j:j+batch_size]:\n",
    "                    A, deltas = self.deltas_dict(p)\n",
    "                    delta_list.append(deltas)\n",
    "                    A_list.append(A)\n",
    "\n",
    "                for i in range(1, self.L):\n",
    "                    self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
    "                    self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
    "            print(f\"{k} Cost = {self.MSE(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e699461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()\n",
    "\n",
    "# Check the shape of the training set\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2c636c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the first matrix in the training set\n",
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26969066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the test set\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dadf57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_X/255\n",
    "test_X = test_X/255\n",
    "train_X[0].flatten().reshape(28*28, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c749e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for x in train_X:\n",
    "    X.append(x.flatten().reshape(28*28, 1))\n",
    "\n",
    "# Y will temp store one-hot encoded label vectors\n",
    "Y = []\n",
    "for y in train_y:\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    Y.append(temp_vec)\n",
    "\n",
    "# Our data will be stored as a list of tuples. \n",
    "train_data = [p for p in zip(X, Y)]\n",
    "\n",
    "# the same method to deal with test data\n",
    "X = []\n",
    "for x in test_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "Y = []\n",
    "for y in test_y:\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    Y.append(temp_vec)\n",
    "\n",
    "test_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09f28b",
   "metadata": {},
   "source": [
    "We will train MLP's using sigmoid, hyperbolic tangent, and rectified linear activation functions by mini batch gradient descent, and compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e07392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.576435207350686\n",
      "0 Cost = 0.17838318571101405\n",
      "1 Cost = 0.15550323442493222\n",
      "2 Cost = 0.14375768223367047\n",
      "3 Cost = 0.13607407914044428\n",
      "4 Cost = 0.13047589072584215\n"
     ]
    }
   ],
   "source": [
    "net_tanh = MultilayerPerceptron(layers=[784, 60, 60, 10], actFun_type='tanh')\n",
    "net_tanh.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23dfeac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13757809796154796"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_tanh.MSE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa31e95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 0.6419399947820255\n",
      "0 Cost = 0.28038644767539217\n",
      "1 Cost = 0.2706564139346542\n",
      "2 Cost = 0.26597957673599076\n",
      "3 Cost = 0.2627727399124922\n",
      "4 Cost = 0.2599156259155153\n"
     ]
    }
   ],
   "source": [
    "net_relu = MultilayerPerceptron(layers=[784, 100, 100, 10], actFun_type='relu')\n",
    "net_relu.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c861fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26594929199648265"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_relu.MSE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea014a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.037490266181791\n",
      "0 Cost = 0.43563291676217736\n",
      "1 Cost = 0.4082627949629969\n",
      "2 Cost = 0.3645917560730352\n",
      "3 Cost = 0.3256380543481417\n",
      "4 Cost = 0.2968073652881028\n"
     ]
    }
   ],
   "source": [
    "net_sig = MultilayerPerceptron(layers=[784, 100, 100, 10], actFun_type='sigmoid')\n",
    "net_sig.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c403eec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2977860616592044"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_sig.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c81b2",
   "metadata": {},
   "source": [
    "### conclusion:  from the output, we think sigmoid activation function has the best perfomance on the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
